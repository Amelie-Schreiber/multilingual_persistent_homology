{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch numpy gudhi -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = [\"In the realm of linguistics, it is crucial to foster a deep understanding of language structures and their interconnections. This understanding is not simply about memorizing vocabulary or grammar rules, but delving into the intricate details of phonetics, semantics, and syntax. However, the common elements that we often focus on, such as morphemes, phonemes, and syntax rules, are merely the tip of the iceberg. Beneath the surface, there lies a vast field of cognitive, social, and historical factors that play significant roles in shaping a language. Grasping these underlying influences is what sets apart a competent linguist from a casual language learner.\"]\n",
    "\n",
    "# Yiddish\n",
    "text_2 = [\"אין דער וועלט פון לינגוויסטיק, איז דער נעציק צו פאָסטערן אַ טיף פֿאַרשטייַנען פון שפּראַך סטרוקטורן און זייערע אינטערקאַנעקשאַנס. דאס פֿאַרשטייַנען איז ניט נאָר וועגן לערנען וואָקאַבולערי אָדער גראַמאַטיק רולז, אָבער אויך ערנטע די דעטאַלז פון פֿאָנעטיק, סעמאַנטיקס, און סינטאַקס. דאָך, די געוויינטלעך עלעמענטן אַז מיר פֿאָקוסירן אויף, ווי מאָרפֿעמען, פֿאָנעמען, און סינטאַקס רולז, זענען בלויז דער שפּיץ פון די ייסבערג. אונטער דער אויבערפֿלאַך, ליגן אַ וואַסט פעלד פון קאָגניטיוו, סאַציאַל, און היסטאָרישע פאַקטאָרן וואָס שפּילן וויכטיק ראָלז אין שייפֿינג אַ שפּראַך. גרייפֿן די אונטערלייגנדיקע אינפֿלוענסז איז וואָס שטעלט אַרויס אַ קאָמפּעטענט לינגוויסט פון אַ קאַזשועל שפּראַך לערנער.\"]\n",
    "\n",
    "# Hebrew\n",
    "text_3 = [\"בעולם של הבלשנות, ההבנה העמוקה של מבני שפה והיחסים שביניהם היא המרכז. ההבנה הזו אינה מתמקדת רק בלמידת אוצר מילים או כללי דקדוק, אלא גם בחקירת פרטי הפונטיקה, הסמנטיקה, והתחביר. אך, הרכיבים הרגילים שאנו מתמקדים בהם, כמו מורפמות, פונמות, וכללים תחביריים, הם רק קצה הקרחון. מתחת לפני השטח, נמצאת שדה עצום של גורמים קוגניטיביים, חברתיים, והיסטוריים שמשחקים תפקידים חשובים בעיצוב שפה. חידוד ההבנה של ההשפעות המשניות האלה הוא מה שמבדיל בין בלשן מיומן ללומד שפה חסר ניסיון.\"]\n",
    "\n",
    "# Russian\n",
    "text_4 = [\"В мире лингвистики глубокое понимание структур языка и их взаимосвязей является ключевым. Это понимание не ограничивается изучением словарного запаса или грамматических правил, но также включает изучение деталей фонетики, семантики и синтаксиса. Однако обычные компоненты, на которые мы обычно сосредотачиваемся, такие как морфемы, фонемы и грамматические правила, это лишь вершина айсберга. Под поверхностью находится огромное поле когнитивных, социальных и исторических факторов, которые играют важную роль в формировании языка. Уточнение понимания этих второстепенных влияний - то, что отличает опытного лингвиста от неопытного изучающего язык.\"]\n",
    "\n",
    "# German\n",
    "text_5 = [\"In der Welt der Linguistik ist ein tiefes Verständnis für Sprachstrukturen und ihre Beziehungen zentral. Dieses Verständnis beschränkt sich nicht nur auf das Erlernen von Vokabeln oder Grammatikregeln, sondern beinhaltet auch die Untersuchung von Details in Phonologie, Semantik und Syntax. Doch die üblichen Komponenten, auf die wir uns konzentrieren, wie Morpheme, Phoneme und Syntaxregeln, sind nur die Spitze des Eisbergs. Unter der Oberfläche liegt ein weites Feld kognitiver, sozialer und historischer Faktoren, die eine wichtige Rolle bei der Formung einer Sprache spielen. Das Verfeinern des Verständnisses dieser sekundären Einflüsse ist es, was einen versierten Linguisten von einem unerfahrenen Sprachlerner unterscheidet.\"]\n",
    "\n",
    "# French\n",
    "text_6 = [\"Dans le monde de la linguistique, une compréhension profonde des structures linguistiques et de leurs relations est centrale. Cette compréhension ne se limite pas à l'apprentissage du vocabulaire ou des règles grammaticales, mais inclut également l'étude des détails de la phonologie, de la sémantique et de la syntaxe. Cependant, les composants usuels sur lesquels nous nous concentrons, tels que les morphèmes, les phonèmes et les règles de syntaxe, ne sont que la pointe de l'iceberg. Sous la surface se trouve un vaste champ de facteurs cognitifs, sociaux et historiques qui jouent un rôle important dans la formation d'une langue. Affiner la compréhension de ces influences secondaires est ce qui distingue un linguiste expérimenté d'un apprenant de langue inexpérimenté.\"]\n",
    "\n",
    "# Bengali\n",
    "text_7 = [\"ভাষাবিজ্ঞানের জগতে ভাষার গঠনমূলগুলি এবং তাদের মধ্যে সম্পর্কের গভীর বোধ কেন্দ্রীয়। এই বোধ শুধু ভোকাবুলারি বা ব্যাকরণের নিয়ম শেখার সাথে সীমাবদ্ধ নয়, বরং ফোনেটিক্স, সেম্যান্টিক্স এবং সিনট্যাক্সের বিস্তারিত অধ্যয়নও অন্তর্ভুক্ত করে। তবে, আমরা যে সাধারণ উপাদানগুলিতে মনোনিবেশ করি, যেমন মর্ফেম, ফোনেম এবং ব্যাকরণের নিয়ম, তা শুধু আইসবার্গের শীর্ষ। পৃষ্ঠতলের নিচে সংজ্ঞানাত্মক, সামাজিক এবং ঐতিহাসিক কারকগুলির একটি বিশাল ক্ষেত্র রয়েছে, যা একটি ভাষার গঠনে গুরুত্বপূর্ণ ভূমিকা পালন করে। এই দ্বিতীয়কক্ষ প্রভাবগুলির বোধ নির্দিষ্ট করা হলেই একজন অভিজ্ঞ ভাষাবিজ্ঞানী এবং অঅভিজ্ঞ ভাষা শিক্ষার্থীর মধ্যে পার্থক্য তৈরি হয়।\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "def compute_output(sentence, layer, head):\n",
    "    # Load pre-trained model\n",
    "    model = BertModel.from_pretrained('bert-base-multilingual-cased', output_attentions=True)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "\n",
    "    # Tokenize input and convert to tensor\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    # Forward pass\n",
    "    # Specify `output_hidden_states=True` when calling the model\n",
    "    outputs = model(**inputs, output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "    # Obtain the attention weights\n",
    "    attentions = outputs.attentions\n",
    "\n",
    "    # Obtain the attention weights for the specific layer and head\n",
    "    S = attentions[layer][0, head]\n",
    "\n",
    "    # Obtain the value vectors\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        hidden_states = outputs.hidden_states[layer]\n",
    "        all_W_v = model.encoder.layer[layer].attention.self.value.weight\n",
    "        num_heads = model.config.num_attention_heads\n",
    "        head_dim = model.config.hidden_size // num_heads\n",
    "        W_v_heads = all_W_v.view(num_heads, head_dim, model.config.hidden_size)\n",
    "        W_v = W_v_heads[head]\n",
    "        V = torch.matmul(hidden_states, W_v.t())\n",
    "\n",
    "    # Compute the output O\n",
    "    O = torch.matmul(S, V)\n",
    "\n",
    "    return O\n",
    "\n",
    "# Set the layer and head to use for computation\n",
    "layer = 5\n",
    "head = 10\n",
    "\n",
    "# Compute the context vectors for each text in the corpus\n",
    "context_1 = [compute_output(t, layer, head) for t in text_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "context_2 = [compute_output(t, layer, head) for t in text_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "context_3 = [compute_output(t, layer, head) for t in text_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "context_4 = [compute_output(t, layer, head) for t in text_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "context_5 = [compute_output(t, layer, head) for t in text_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "context_6 = [compute_output(t, layer, head) for t in text_6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "context_7 = [compute_output(t, layer, head) for t in text_7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "import gudhi as gd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "def compute_phrase_distances_and_homology(context_vectors, sentence, phrase):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "    # Tokenize the sentence and the phrase\n",
    "    sentence_tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "    phrase_tokens = tokenizer.encode(phrase, add_special_tokens=False)\n",
    "\n",
    "    # Find the indices of the phrase tokens in the sentence\n",
    "    phrase_indices = []\n",
    "    phrase_length = len(phrase_tokens)\n",
    "    for i in range(len(sentence_tokens) - phrase_length + 1):\n",
    "        if sentence_tokens[i:i+phrase_length] == phrase_tokens:\n",
    "            phrase_indices.extend(range(i, i+phrase_length))\n",
    "            break\n",
    "\n",
    "    # Extract the context vectors for the phrase\n",
    "    phrase_context_vectors = context_vectors[0, phrase_indices]\n",
    "\n",
    "    # Detach the tensor and convert to numpy array\n",
    "    phrase_context_vectors_np = phrase_context_vectors.detach().numpy()\n",
    "\n",
    "    # Print the tokens of the sub-collection and their context vectors\n",
    "    # print(f'Tokens of the sub-collection: {tokenizer.convert_ids_to_tokens(phrase_tokens)}')\n",
    "    # print(f'Context vectors of the sub-collection: {phrase_context_vectors_np}')\n",
    "\n",
    "    # Compute the pairwise Euclidean distances among the phrase context vectors\n",
    "    distances = distance_matrix(phrase_context_vectors_np, phrase_context_vectors_np)\n",
    "\n",
    "    # Print the distance matrix\n",
    "    # print(f'Distance matrix: {distances.shape}')\n",
    "    # print(f'Distance matrix: {distances}')\n",
    "\n",
    "    # Compute the persistent homology of the distance matrix\n",
    "    rips_complex = gd.RipsComplex(distance_matrix=distances, max_edge_length=np.max(distances))\n",
    "    simplex_tree = rips_complex.create_simplex_tree(max_dimension=2)\n",
    "    persistent_homology = simplex_tree.persistence(min_persistence=0.001)\n",
    "\n",
    "    # Plot the barcode diagram\n",
    "    # gd.plot_persistence_barcode(persistence=persistent_homology)\n",
    "    # plt.show()\n",
    "\n",
    "    return persistent_homology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wasserstein distance: = 20.48\n"
     ]
    }
   ],
   "source": [
    "from gudhi.hera import wasserstein_distance\n",
    "import numpy as np\n",
    "\n",
    "persistent_homology_1 = compute_phrase_distances_and_homology(context_1[0], text_1[0], \"the tip of the iceberg\")\n",
    "persistent_homology_np_1 = np.array([[pt[1][0], pt[1][1]] for pt in persistent_homology_1 if pt[1][1] != float('inf')])\n",
    "\n",
    "persistent_homology_2 = compute_phrase_distances_and_homology(context_2[0], text_2[0], \"דער שפּיץ פון די ייסבערג\")\n",
    "persistent_homology_np_2 = np.array([[pt[1][0], pt[1][1]] for pt in persistent_homology_2 if pt[1][1] != float('inf')])\n",
    "\n",
    "print(f\"Wasserstein distance: = {wasserstein_distance(persistent_homology_np_1, persistent_homology_np_2, order=1., internal_p=2.):.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wasserstein distance: = 1.89\n"
     ]
    }
   ],
   "source": [
    "\n",
    "persistent_homology_3 = compute_phrase_distances_and_homology(context_3[0], text_3[0], \"קצה הקרחון\")\n",
    "persistent_homology_np_3 = np.array([[pt[1][0], pt[1][1]] for pt in persistent_homology_3 if pt[1][1] != float('inf')])\n",
    "\n",
    "print(f\"Wasserstein distance: = {wasserstein_distance(persistent_homology_np_1, persistent_homology_np_3, order=1., internal_p=2.):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wasserstein distance: = 1.64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "persistent_homology_4 = compute_phrase_distances_and_homology(context_4[0], text_4[0], \"вершина айсберга\")\n",
    "persistent_homology_np_4 = np.array([[pt[1][0], pt[1][1]] for pt in persistent_homology_4 if pt[1][1] != float('inf')])\n",
    "\n",
    "print(f\"Wasserstein distance: = {wasserstein_distance(persistent_homology_np_1, persistent_homology_np_4, order=1., internal_p=2.):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wasserstein distance: = 3.42\n"
     ]
    }
   ],
   "source": [
    "\n",
    "persistent_homology_5 = compute_phrase_distances_and_homology(context_5[0], text_5[0], \"die Spitze des Eisbergs\")\n",
    "persistent_homology_np_5 = np.array([[pt[1][0], pt[1][1]] for pt in persistent_homology_5 if pt[1][1] != float('inf')])\n",
    "\n",
    "print(f\"Wasserstein distance: = {wasserstein_distance(persistent_homology_np_1, persistent_homology_np_5, order=1., internal_p=2.):.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wasserstein distance: = 4.99\n"
     ]
    }
   ],
   "source": [
    "\n",
    "persistent_homology_6 = compute_phrase_distances_and_homology(context_6[0], text_6[0], \"la pointe de l'iceberg\")\n",
    "persistent_homology_np_6 = np.array([[pt[1][0], pt[1][1]] for pt in persistent_homology_6 if pt[1][1] != float('inf')])\n",
    "\n",
    "print(f\"Wasserstein distance: = {wasserstein_distance(persistent_homology_np_1, persistent_homology_np_6, order=1., internal_p=2.):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wasserstein distance: = 15.12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "persistent_homology_7 = compute_phrase_distances_and_homology(context_7[0], text_7[0], \"আইসবার্গের শীর্ষ\")\n",
    "persistent_homology_np_7 = np.array([[pt[1][0], pt[1][1]] for pt in persistent_homology_7 if pt[1][1] != float('inf')])\n",
    "\n",
    "print(f\"Wasserstein distance: = {wasserstein_distance(persistent_homology_np_1, persistent_homology_np_7, order=1., internal_p=2.):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
