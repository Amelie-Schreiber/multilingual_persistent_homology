{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistent Homology of Collocations and Multiword Expressions Across Contexts for Hebrew"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for comparing how well two models preserves the persistent homology of some keyphrase like a collocation, multiword expression, or idiom. We take such a keyphrase and place it in several different contexts `text[i]`, then compute the persistent homology of the context vectors in each context. Once this is complete, we compute the pairwise distances between the persistent diagrams using the Wasserstein distance metric. This gives two distance matrices, one for each mode (here we use `xlm-roberta-large` and `TurkuNLP/wikibert-base-he-cased`). Once we have computed this distance matrix, we do a simple elementwise comparison to see what percentage of the elements in the difference of the two distance matrices are negative. Note that this is a per attention head comparison, so we can compare different attention heads in each model. Changing the second function `compute_output_b()` to use the first model `xlm-roberta-large` allows for comparing different attention heads *within the same model*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch numpy gudhi -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"בחופשה האחרונה שלנו, נסענו להכיר את היופיים של מדבר הנגב. בין אם מדובר בצוקים המרשימים, בחי הבר המיוחד, או בשקט המוחלט, יש משהו מאוד מיוחד במדבר. אחת החוויות המרגשות ביותר שלנו הייתה לצפות בזרחת השמש מעל המדבר. האור המתפשט מאחורי ההרים, השמים המשתנים מאוד מהיר מאופל לתכלת, והשלווה המוחלטת שאפשר רק במדבר, הכל הפך את החוויה לבלתי נשכחת\", \n",
    "          \n",
    "          \"במהלך שנת הלימודים הראשונה שלי באוניברסיטה, הצטרפתי לקבוצת טיול שהגיעה להר האייפל. למרות הקור החודר, הייתי מחויב לעלות לפסגה בכל בוקר, כדי לצפות בזרחת השמש מעל פריס\", \n",
    "          \n",
    "          \"הייתי מתעורר בבוקר מוקדם, לפני כולם, כדי לצפות בזרחת השמש. היו ימים שהשמים היו מלאים בגוונים של ורוד וכתום, והאוויר הקר היה ממלא את הריאות. הייתי נושם את השקט, מאזין לשירת הציפורים, ומרגיש את היום החדש שמתחיל.\", \n",
    "          \n",
    "          \"הייתה לי הרגל לצפות בזרחת השמש כאשר הייתי טסה למקומות רחוקים. הייתי מתמקדת באור הזהב של השמש שהתפשט על פני האופק. זה היה רגע של שקט ושלווה, שבו הייתי מרגישה את האפשרויות של היום שלפני.\", \n",
    "          \n",
    "          \"אחד הדברים שאני ממש אוהב לעשות בחופשות הוא לצפות בזרחת השמש על גג המלון. אין דבר יותר מרגיע מאשר לשבת עם כוס קפה ביד, להתבונן בנוף, ולראות איך העולם מתעורר לחיים.\", \n",
    "          \n",
    "          \"במהלך ההליכה, עצרנו לרגע כדי לצפות בזרחת השמש. האור הראשוני של היום הזהיר את השמיים בצבעים של זהב, ואנחנו ישבנו שם בשקט, מתפללים ליום טוב.\", \n",
    "          \n",
    "          \"אני מאמין שאין דבר מרגש יותר מ לצפות בזרחת השמש. כאשר האור הראשונים מתחילים להתפשט באופק, אתה מרגיש כאילו אתה חלק ממשהו גדול מאוד. זה מזכיר לי כמה העולם הזה גדול ויפה.\", \n",
    "          \n",
    "          \"אחת הפעמים המיוחדות ביותר שבהן הזמנתי לצפות בזרחת השמש הייתה בחופשה שלי בהודו. הייתי מתעורר מוקדם, לפני כל העולם, ומשתקף למראה המרהיבה של השמש המתעלה מעל האוקיינוס. זה היה חוויה שאני לעולם לא אשכח.\", \n",
    "          \n",
    "          \"אני אוהב לצפות בזרחת השמש מהחלון שלי. זה נותן לי את האנרגיה להתחיל את היום. אני אפילו מקדיש כמה דקות בכל בוקר לקחת כוס קפה, לשבת מול החלון, ולהשתקף במראה המדהים הזה.\", \n",
    "          \n",
    "          \"בראשית, אני אוהב לצפות בזרחת השמש מהמרפסת שלי. זה מזכיר לי את היופי של העולם, את התקווה של יום חדש, ואת החיים הממשיכים להתפתח בכל יום. כל זריחה מציגה תמונה שונה, נוף חדש שממלא אותי בתחושת התרגשות והתפעלות.\", \n",
    "          \n",
    "          \"היום האחרון שלי ביפן היה יום מיוחד. יצאתי להליך מוקדם בבוקר, כדי לצפות בזרחת השמש מעל הר הפוג'. האור הראשוני של היום מאיר את השיחים המקופים בשלג, מצייר תמונה יפהפיה שאני לעולם לא אשכח.\", \n",
    "          \n",
    "          \"אחד החוויות המרגשות ביותר שלי היתה לצפות בזרחת השמש מעל הפירמידות במצרים. האור החום החודר את השחקים, מאיר את האבנים העתיקות, ומעניק להם מראה של זהב. זה היה רגע של התבוננות והתפעלות על ההיסטוריה שלנו.\", \n",
    "          \n",
    "          \"כאשר אני מטייל בים, אני מתכנן לצפות בזרחת השמש מהחוף. אין דבר מרהיב יותר מלראות את האור הראשון של היום מתפשט על גלי הים, משנה את צבעם לגוונים של זהב ואורנג'. זה הופך את החוויה של ההליכה למשהו יוצא דופן.\", \n",
    "          \n",
    "          \"האלפים היה חלום שלי. כשהגעתי לשם, הייתי עייף אבל מרוצה. לצפות בזרחת השמש מהפסגה, כשהאור העדין של השחר התחיל להתפשט על השפעי השלג הלבנים, היה חוויה בלתי נשכחת. העולם התמלא בנופים שלא ראיתי מעולם. זה היה מרגע של שלווה ושקט, שהיה שווה את כל המאמץ.\", \n",
    "          \n",
    "          \"אחרי שהוא התעורר מהשנה, איזיק הכין לעצמו כוס קפה והולך להסתובב בגן הפרטי שלו. זו הייתה הדרך האהובה עליו להתחיל את היום - לצפות בזרחת השמש ולשמוע את הציפורים מצפצפות.\", \n",
    "          \n",
    "          \"את חייבת לצפות בזרחת השמש מהחוף שלנו, אמר יגאל למרים, כאשר הם הגיעו לבית הנופש של המשפחה. זו תחוויה בלתי נשכחת, משהו שתזכורי לעוד שנים.\",\n",
    "          \n",
    "          \"בזמן שהכל בעיר עדיין ישן, רבקה מתעוררת בשעה המוקדמת ביותר שאפשר, מתארגנת, ויוצאת לרוץ. היא אוהבת את השקט של אותן שעות, והמראה של העיר שמתעוררת לחיים. אבל מעל כל, זה הזמן היחיד שהיא יכולה לצפות בזרחת השמש בלי להיות מופרעת.\",\n",
    "\n",
    "          \"לשפת האגם הגיעה מיה, המצלמה שלה כבר מוכנה לפעולה. היא התיישבה בשקט, מצפה לרגע הנכון. היא יודעת שממש בקרוב, היא תהיה מסוגלת לצפות בזרחת השמש, והיא רוצה לתפוס את הרגע המושלם בתמונה.\"\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 בחופשה האחרונה שלנו, נסענו להכיר את היופיים של מדבר הנגב. בין אם מדובר בצוקים המרשימים, בחי הבר המיוחד, או בשקט המוחלט, יש משהו מאוד מיוחד במדבר. אחת החוויות המרגשות ביותר שלנו הייתה לצפות בזרחת השמש מעל המדבר. האור המתפשט מאחורי ההרים, השמים המשתנים מאוד מהיר מאופל לתכלת, והשלווה המוחלטת שאפשר רק במדבר, הכל הפך את החוויה לבלתי נשכחת\n",
      "1 במהלך שנת הלימודים הראשונה שלי באוניברסיטה, הצטרפתי לקבוצת טיול שהגיעה להר האייפל. למרות הקור החודר, הייתי מחויב לעלות לפסגה בכל בוקר, כדי לצפות בזרחת השמש מעל פריס\n",
      "2 הייתי מתעורר בבוקר מוקדם, לפני כולם, כדי לצפות בזרחת השמש. היו ימים שהשמים היו מלאים בגוונים של ורוד וכתום, והאוויר הקר היה ממלא את הריאות. הייתי נושם את השקט, מאזין לשירת הציפורים, ומרגיש את היום החדש שמתחיל.\n",
      "3 הייתה לי הרגל לצפות בזרחת השמש כאשר הייתי טסה למקומות רחוקים. הייתי מתמקדת באור הזהב של השמש שהתפשט על פני האופק. זה היה רגע של שקט ושלווה, שבו הייתי מרגישה את האפשרויות של היום שלפני.\n",
      "4 אחד הדברים שאני ממש אוהב לעשות בחופשות הוא לצפות בזרחת השמש על גג המלון. אין דבר יותר מרגיע מאשר לשבת עם כוס קפה ביד, להתבונן בנוף, ולראות איך העולם מתעורר לחיים.\n",
      "5 במהלך ההליכה, עצרנו לרגע כדי לצפות בזרחת השמש. האור הראשוני של היום הזהיר את השמיים בצבעים של זהב, ואנחנו ישבנו שם בשקט, מתפללים ליום טוב.\n",
      "6 אני מאמין שאין דבר מרגש יותר מ לצפות בזרחת השמש. כאשר האור הראשונים מתחילים להתפשט באופק, אתה מרגיש כאילו אתה חלק ממשהו גדול מאוד. זה מזכיר לי כמה העולם הזה גדול ויפה.\n",
      "7 אחת הפעמים המיוחדות ביותר שבהן הזמנתי לצפות בזרחת השמש הייתה בחופשה שלי בהודו. הייתי מתעורר מוקדם, לפני כל העולם, ומשתקף למראה המרהיבה של השמש המתעלה מעל האוקיינוס. זה היה חוויה שאני לעולם לא אשכח.\n",
      "8 אני אוהב לצפות בזרחת השמש מהחלון שלי. זה נותן לי את האנרגיה להתחיל את היום. אני אפילו מקדיש כמה דקות בכל בוקר לקחת כוס קפה, לשבת מול החלון, ולהשתקף במראה המדהים הזה.\n",
      "9 בראשית, אני אוהב לצפות בזרחת השמש מהמרפסת שלי. זה מזכיר לי את היופי של העולם, את התקווה של יום חדש, ואת החיים הממשיכים להתפתח בכל יום. כל זריחה מציגה תמונה שונה, נוף חדש שממלא אותי בתחושת התרגשות והתפעלות.\n",
      "10 היום האחרון שלי ביפן היה יום מיוחד. יצאתי להליך מוקדם בבוקר, כדי לצפות בזרחת השמש מעל הר הפוג'. האור הראשוני של היום מאיר את השיחים המקופים בשלג, מצייר תמונה יפהפיה שאני לעולם לא אשכח.\n",
      "11 אחד החוויות המרגשות ביותר שלי היתה לצפות בזרחת השמש מעל הפירמידות במצרים. האור החום החודר את השחקים, מאיר את האבנים העתיקות, ומעניק להם מראה של זהב. זה היה רגע של התבוננות והתפעלות על ההיסטוריה שלנו.\n",
      "12 כאשר אני מטייל בים, אני מתכנן לצפות בזרחת השמש מהחוף. אין דבר מרהיב יותר מלראות את האור הראשון של היום מתפשט על גלי הים, משנה את צבעם לגוונים של זהב ואורנג'. זה הופך את החוויה של ההליכה למשהו יוצא דופן.\n",
      "13 האלפים היה חלום שלי. כשהגעתי לשם, הייתי עייף אבל מרוצה. לצפות בזרחת השמש מהפסגה, כשהאור העדין של השחר התחיל להתפשט על השפעי השלג הלבנים, היה חוויה בלתי נשכחת. העולם התמלא בנופים שלא ראיתי מעולם. זה היה מרגע של שלווה ושקט, שהיה שווה את כל המאמץ.\n",
      "14 אחרי שהוא התעורר מהשנה, איזיק הכין לעצמו כוס קפה והולך להסתובב בגן הפרטי שלו. זו הייתה הדרך האהובה עליו להתחיל את היום - לצפות בזרחת השמש ולשמוע את הציפורים מצפצפות.\n",
      "15 את חייבת לצפות בזרחת השמש מהחוף שלנו, אמר יגאל למרים, כאשר הם הגיעו לבית הנופש של המשפחה. זו תחוויה בלתי נשכחת, משהו שתזכורי לעוד שנים.\n",
      "16 בזמן שהכל בעיר עדיין ישן, רבקה מתעוררת בשעה המוקדמת ביותר שאפשר, מתארגנת, ויוצאת לרוץ. היא אוהבת את השקט של אותן שעות, והמראה של העיר שמתעוררת לחיים. אבל מעל כל, זה הזמן היחיד שהיא יכולה לצפות בזרחת השמש בלי להיות מופרעת.\n",
      "17 לשפת האגם הגיעה מיה, המצלמה שלה כבר מוכנה לפעולה. היא התיישבה בשקט, מצפה לרגע הנכון. היא יודעת שממש בקרוב, היא תהיה מסוגלת לצפות בזרחת השמש, והיא רוצה לתפוס את הרגע המושלם בתמונה.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(text)):\n",
    "    print(i, text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "from transformers import AutoTokenizer, AutoModel  # Corrected import here\n",
    "\n",
    "def compute_output(sentence, layer, head):\n",
    "    # Load pre-trained model\n",
    "    tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large')\n",
    "    model = AutoModel.from_pretrained(\"xlm-roberta-large\", output_attentions=True) \n",
    "\n",
    "    # Tokenize input and convert to tensor\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    # Forward pass\n",
    "    # Specify `output_hidden_states=True` when calling the model\n",
    "    outputs = model(**inputs, output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "    # Obtain the attention weights\n",
    "    attentions = outputs.attentions\n",
    "\n",
    "    # Obtain the attention weights for the specific layer and head\n",
    "    S = attentions[layer][0, head]\n",
    "\n",
    "    # Obtain the value vectors\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        hidden_states = outputs.hidden_states[layer]\n",
    "        all_W_v = model.encoder.layer[layer].attention.self.value.weight\n",
    "        num_heads = model.config.num_attention_heads\n",
    "        head_dim = model.config.hidden_size // num_heads\n",
    "        W_v_heads = all_W_v.view(num_heads, head_dim, model.config.hidden_size)\n",
    "        W_v = W_v_heads[head]\n",
    "        V = torch.matmul(hidden_states, W_v.t())\n",
    "\n",
    "    # Compute the output O\n",
    "    O = torch.matmul(S, V)\n",
    "\n",
    "    return O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Set the layer and head to use for computation\n",
    "layer = 7\n",
    "head = 3\n",
    "\n",
    "context = []\n",
    "for i in range(len(text)):\n",
    "    context.append(compute_output(text[i], layer, head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "import gudhi as gd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "def compute_phrase_distances_and_homology(context_vectors, sentence, phrase):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large')\n",
    "\n",
    "    # Tokenize the sentence and the phrase\n",
    "    sentence_tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "    phrase_tokens = tokenizer.encode(phrase, add_special_tokens=False)\n",
    "\n",
    "    # Find the indices of the phrase tokens in the sentence\n",
    "    phrase_indices = []\n",
    "    phrase_length = len(phrase_tokens)\n",
    "    for i in range(len(sentence_tokens) - phrase_length + 1):\n",
    "        if sentence_tokens[i:i+phrase_length] == phrase_tokens:\n",
    "            phrase_indices.extend(range(i, i+phrase_length))\n",
    "            break\n",
    "\n",
    "    # Extract the context vectors for the phrase\n",
    "    phrase_context_vectors = context_vectors[0, phrase_indices]\n",
    "\n",
    "    # Detach the tensor and convert to numpy array\n",
    "    phrase_context_vectors_np = phrase_context_vectors.detach().numpy()\n",
    "\n",
    "    # Print the tokens of the sub-collection and their context vectors\n",
    "    # print(f'Tokens of the sub-collection: {tokenizer.convert_ids_to_tokens(phrase_tokens)}')\n",
    "    # print(f'Context vectors of the sub-collection: {phrase_context_vectors_np}')\n",
    "\n",
    "    # Compute the pairwise Euclidean distances among the phrase context vectors\n",
    "    distances = distance_matrix(phrase_context_vectors_np, phrase_context_vectors_np)\n",
    "\n",
    "    # Print the distance matrix\n",
    "    # print(f'Distance matrix: {distances.shape}')\n",
    "    # print(f'Distance matrix: {distances}')\n",
    "\n",
    "    # Compute the persistent homology of the distance matrix\n",
    "    rips_complex = gd.RipsComplex(distance_matrix=distances, max_edge_length=np.max(distances))\n",
    "    simplex_tree = rips_complex.create_simplex_tree(max_dimension=2)\n",
    "    persistent_homology = simplex_tree.persistence(min_persistence=0.001)\n",
    "\n",
    "    # Plot the barcode diagram\n",
    "    # gd.plot_persistence_barcode(persistence=persistent_homology)\n",
    "    # plt.show()\n",
    "\n",
    "    return persistent_homology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "persistent_homology = []\n",
    "for i in range(len(text)):\n",
    "    persistent_homology.append(compute_phrase_distances_and_homology(context[i], text[i], \"לצפות בזרחת השמש\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gudhi.hera import wasserstein_distance\n",
    "import numpy as np\n",
    "\n",
    "def transform_persistence_diagram(diagram):\n",
    "    # Remove the dimension from each feature and return the transformed diagram\n",
    "    return [(birth, death) for dimension, (birth, death) in diagram]\n",
    "\n",
    "def compute_wasserstein_distances(persistence_diagrams, p=2):\n",
    "    n = len(persistence_diagrams)\n",
    "    distances = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            diagram1 = transform_persistence_diagram(persistence_diagrams[i])\n",
    "            diagram2 = transform_persistence_diagram(persistence_diagrams[j])\n",
    "            distance = wasserstein_distance(diagram1, diagram2, order=1., internal_p=2.)\n",
    "            distances[i, j] = distance\n",
    "            distances[j, i] = distance\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "persistence_diagrams_1 = []\n",
    "for i in range(len(text)):\n",
    "    persistence_diagrams_1.append(persistent_homology[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.14358496 0.70826325 0.4711087  0.46510486 0.66519472\n",
      "  0.72106172 0.64639516 0.25645569 0.74026037 0.57935826 0.66323981\n",
      "  0.65809475 0.50263554 0.33131627 0.75071186 0.62353368 0.6456653 ]\n",
      " [0.14358496 0.         0.6067536  0.38272286 0.46688104 0.52160976\n",
      "  0.63267588 0.53151107 0.30987864 0.78782533 0.50614469 0.68760714\n",
      "  0.65631857 0.47948699 0.36945366 0.66232602 0.58357096 0.64388911]\n",
      " [0.70826325 0.6067536  0.         0.51532513 0.87037348 0.35806367\n",
      "  0.24278376 0.17987196 0.63386834 0.35499558 0.128905   1.15788785\n",
      "  0.17214855 0.39345292 0.62027882 0.42893342 0.15904181 0.19465545]\n",
      " [0.4711087  0.38272286 0.51532513 0.         0.46565468 0.25949316\n",
      "  0.34015762 0.37705579 0.39888494 0.73971055 0.48912666 0.89639451\n",
      "  0.65414876 0.35109556 0.2456747  0.30970259 0.4746897  0.64511548]\n",
      " [0.46510486 0.46688104 0.87037348 0.46565468 0.         0.66451876\n",
      "  0.77979867 0.82024867 0.6845074  1.19196901 0.91484238 0.82375536\n",
      "  1.00976632 0.79432513 0.57246977 0.59364901 0.91906091 1.00319075]\n",
      " [0.66519472 0.52160976 0.35806367 0.25949316 0.66451876 0.\n",
      "  0.35916785 0.30416063 0.61850305 0.68745567 0.40577503 1.06349415\n",
      "  0.48808933 0.26489945 0.44977085 0.23798432 0.45382488 0.48251813]\n",
      " [0.72106172 0.63267588 0.24278376 0.34015762 0.77979867 0.35916785\n",
      "  0.         0.12964598 0.6373169  0.42556656 0.2096506  1.20324699\n",
      "  0.34340094 0.54265367 0.55679613 0.21830015 0.13926224 0.34742166]\n",
      " [0.64639516 0.53151107 0.17987196 0.37705579 0.82024867 0.30416063\n",
      "  0.12964598 0.         0.59970349 0.38329504 0.11207087 1.11285527\n",
      "  0.28048914 0.41300769 0.50314305 0.24906146 0.14966425 0.26805969]\n",
      " [0.25645569 0.30987864 0.63386834 0.39888494 0.6845074  0.61850305\n",
      "  0.6373169  0.59970349 0.         0.52085783 0.50496334 0.9196955\n",
      "  0.52718997 0.42824063 0.22745431 0.66696704 0.52470524 0.4569462 ]\n",
      " [0.74026037 0.78782533 0.35499558 0.73971055 1.19196901 0.68745567\n",
      "  0.42556656 0.38329504 0.52085783 0.         0.28168064 1.27787448\n",
      "  0.19936634 0.58167421 0.63289546 0.61171622 0.28630432 0.20493754]\n",
      " [0.57935826 0.50614469 0.128905   0.48912666 0.91484238 0.40577503\n",
      "  0.2096506  0.11207087 0.50496334 0.28168064 0.         1.17174728\n",
      "  0.17575068 0.38475439 0.51543343 0.36113233 0.10059956 0.16321393]\n",
      " [0.66323981 0.68760714 1.15788785 0.89639451 0.82375536 1.06349415\n",
      "  1.20324699 1.11285527 0.9196955  1.27787448 1.17174728 0.\n",
      "  1.15789419 1.01613031 0.87788424 1.17599767 1.20724112 1.112256  ]\n",
      " [0.65809475 0.65631857 0.17214855 0.65414876 1.00976632 0.48808933\n",
      "  0.34340094 0.28048914 0.52718997 0.19936634 0.17575068 1.15789419\n",
      "  0.         0.40755475 0.57476689 0.5295506  0.2041387  0.07582898]\n",
      " [0.50263554 0.47948699 0.39345292 0.35109556 0.79432513 0.26489945\n",
      "  0.54265367 0.41300769 0.42824063 0.58167421 0.38475439 1.01613031\n",
      "  0.40755475 0.         0.28630646 0.46474773 0.4451524  0.395018  ]\n",
      " [0.33131627 0.36945366 0.62027882 0.2456747  0.57246977 0.44977085\n",
      "  0.55679613 0.50314305 0.22745431 0.63289546 0.51543343 0.87788424\n",
      "  0.57476689 0.28630646 0.         0.47176193 0.47615237 0.55675222]\n",
      " [0.75071186 0.66232602 0.42893342 0.30970259 0.59364901 0.23798432\n",
      "  0.21830015 0.24906146 0.66696704 0.61171622 0.36113233 1.17599767\n",
      "  0.5295506  0.46474773 0.47176193 0.         0.3254119  0.51712115]\n",
      " [0.62353368 0.58357096 0.15904181 0.4746897  0.91906091 0.45382488\n",
      "  0.13926224 0.14966425 0.52470524 0.28630432 0.10059956 1.20724112\n",
      "  0.2041387  0.4451524  0.47615237 0.3254119  0.         0.23481   ]\n",
      " [0.6456653  0.64388911 0.19465545 0.64511548 1.00319075 0.48251813\n",
      "  0.34742166 0.26805969 0.4569462  0.20493754 0.16321393 1.112256\n",
      "  0.07582898 0.395018   0.55675222 0.51712115 0.23481    0.        ]]\n"
     ]
    }
   ],
   "source": [
    "w_distances = compute_wasserstein_distances(persistence_diagrams_1)\n",
    "print(w_distances)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "\n",
    "def compute_output_b(sentence, layer, head):\n",
    "    # Load pre-trained model\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained(\"TurkuNLP/wikibert-base-he-cased\")\n",
    "    model = transformers.BertModel.from_pretrained(\"TurkuNLP/wikibert-base-he-cased\", output_attentions=True)\n",
    "\n",
    "\n",
    "    # Tokenize input and convert to tensor\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    # Forward pass\n",
    "    # Specify `output_hidden_states=True` when calling the model\n",
    "    outputs = model(**inputs, output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "    # Obtain the attention weights\n",
    "    attentions = outputs.attentions\n",
    "\n",
    "    # Obtain the attention weights for the specific layer and head\n",
    "    S = attentions[layer][0, head]\n",
    "\n",
    "    # Obtain the value vectors\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        hidden_states = outputs.hidden_states[layer]\n",
    "        all_W_v = model.encoder.layer[layer].attention.self.value.weight\n",
    "        num_heads = model.config.num_attention_heads\n",
    "        head_dim = model.config.hidden_size // num_heads\n",
    "        W_v_heads = all_W_v.view(num_heads, head_dim, model.config.hidden_size)\n",
    "        W_v = W_v_heads[head]\n",
    "        V = torch.matmul(hidden_states, W_v.t())\n",
    "\n",
    "    # Compute the output O\n",
    "    O = torch.matmul(S, V)\n",
    "\n",
    "    return O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of the model checkpoint at TurkuNLP/wikibert-base-he-cased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of the model checkpoint at TurkuNLP/wikibert-base-he-cased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of the model checkpoint at TurkuNLP/wikibert-base-he-cased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of the model checkpoint at TurkuNLP/wikibert-base-he-cased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of the model checkpoint at TurkuNLP/wikibert-base-he-cased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of the model checkpoint at TurkuNLP/wikibert-base-he-cased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of the model checkpoint at TurkuNLP/wikibert-base-he-cased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of the model checkpoint at TurkuNLP/wikibert-base-he-cased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of the model checkpoint at TurkuNLP/wikibert-base-he-cased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of the model checkpoint at TurkuNLP/wikibert-base-he-cased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of the model checkpoint at TurkuNLP/wikibert-base-he-cased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of the model checkpoint at TurkuNLP/wikibert-base-he-cased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of the model checkpoint at TurkuNLP/wikibert-base-he-cased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of the model checkpoint at TurkuNLP/wikibert-base-he-cased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of the model checkpoint at TurkuNLP/wikibert-base-he-cased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of the model checkpoint at TurkuNLP/wikibert-base-he-cased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of the model checkpoint at TurkuNLP/wikibert-base-he-cased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of the model checkpoint at TurkuNLP/wikibert-base-he-cased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Set the layer and head to use for computation\n",
    "layer = 7\n",
    "head = 3\n",
    "\n",
    "context_b = []\n",
    "for i in range(len(text)):\n",
    "    context_b.append(compute_output_b(text[i], layer, head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "persistent_homology_b = []\n",
    "for i in range(len(text)):\n",
    "    persistent_homology_b.append(compute_phrase_distances_and_homology(context_b[i], text[i], \"לצפות בזרחת השמש\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "persistence_diagrams_1b = []\n",
    "for i in range(len(text)):\n",
    "    persistence_diagrams_1b.append(persistent_homology_b[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         3.31376767 4.44075047 2.02051601 1.35515904 4.79234373\n",
      "  5.32450433 1.26935452 3.58035912 1.58135222 4.64680879 2.67487106\n",
      "  2.9142429  2.92964845 3.66830721 1.87328782 5.36017682 6.99961064]\n",
      " [3.31376767 0.         1.17177497 1.92164634 4.08763647 2.52107036\n",
      "  2.13508821 2.37801159 0.62744578 2.25045337 2.66758588 1.12751091\n",
      "  1.94931114 4.20132851 1.14733194 4.77353281 2.09606084 4.58255541]\n",
      " [4.44075047 1.17177497 0.         2.49276498 5.21461927 1.34929538\n",
      "  1.26917333 3.22743344 1.21563762 2.93192877 2.29844461 1.83840993\n",
      "  1.89959226 5.06724339 0.84497378 5.90051561 0.92428586 3.41078043]\n",
      " [2.02051601 1.92164634 2.49276498 0.         3.10089838 2.77182772\n",
      "  3.45810769 1.07400481 2.00414379 0.95961259 4.22677255 1.57347167\n",
      "  1.3273479  4.43967143 2.60621812 3.65760146 3.383868   4.97909463]\n",
      " [1.35515904 4.08763647 5.21461927 3.10089838 0.         5.8727261\n",
      "  6.09837313 2.44706886 4.35422792 2.6617346  5.75384055 3.75525343\n",
      "  3.99462527 2.93117481 4.74868958 1.81762611 6.13404561 8.07999302]\n",
      " [4.79234373 2.52107036 1.34929538 2.77182772 5.8727261  0.\n",
      "  1.13150362 3.52298921 2.32798171 3.2109915  2.91440506 2.11747267\n",
      "  1.99320756 5.34630613 2.11148569 6.42942918 0.81684487 2.20726691]\n",
      " [5.32450433 2.13508821 1.26917333 3.45810769 6.09837313 1.13150362\n",
      "  0.         4.11118731 1.74674115 3.85248462 2.14755786 3.0111519\n",
      "  2.78525378 5.86416697 2.11414712 6.78426948 0.4585373  2.59315409]\n",
      " [1.26935452 2.37801159 3.22743344 1.07400481 2.44706886 3.52298921\n",
      "  4.11118731 0.         2.80155094 0.50022692 3.7827299  1.50429763\n",
      "  1.64488838 3.48008706 2.39895269 2.90643997 4.13502948 5.73025612]\n",
      " [3.58035912 0.62744578 1.21563762 2.00414379 4.35422792 2.32798171\n",
      "  1.74674115 2.80155094 0.         2.73694058 3.13161858 1.68593907\n",
      "  2.22282297 4.60740176 1.77477772 5.04012427 1.94321825 4.33989524]\n",
      " [1.58135222 2.25045337 2.93192877 0.95961259 2.6617346  3.2109915\n",
      "  3.85248462 0.50022692 2.73694058 0.         3.81562904 1.16737906\n",
      "  1.50809181 3.85040784 2.28618619 3.21843767 3.82303178 5.41825842]\n",
      " [4.64680879 2.66758588 2.29844461 4.22677255 5.75384055 2.91440506\n",
      "  2.14755786 3.7827299  3.13161858 3.81562904 0.         3.10292839\n",
      "  3.35342624 4.91070634 1.78300874 6.00366358 2.26087161 4.34577464]\n",
      " [2.67487106 1.12751091 1.83840993 1.57347167 3.75525343 2.11747267\n",
      "  3.0111519  1.50429763 1.68593907 1.16737906 3.10292839 0.\n",
      "  1.49775325 3.22883346 1.55312838 4.31195651 2.72951295 4.32473958]\n",
      " [2.9142429  1.94931114 1.89959226 1.3273479  3.99462527 1.99320756\n",
      "  2.78525378 1.64488838 2.22282297 1.50809181 3.35342624 1.49775325\n",
      "  0.         4.22682078 1.77310621 4.55132835 2.54657372 4.14180036]\n",
      " [2.92964845 4.20132851 5.06724339 4.43967143 2.93117481 5.34630613\n",
      "  5.86416697 3.48008706 4.60740176 3.85040784 4.91070634 3.22883346\n",
      "  4.22682078 0.         4.22226961 1.57678137 5.90649367 7.55357304]\n",
      " [3.66830721 1.14733194 0.84497378 2.60621812 4.74868958 2.11148569\n",
      "  2.11414712 2.39895269 1.77477772 2.28618619 1.78300874 1.55312838\n",
      "  1.77310621 4.22226961 0.         5.12620195 1.7360768  4.17297074]\n",
      " [1.87328782 4.77353281 5.90051561 3.65760146 1.81762611 6.42942918\n",
      "  6.78426948 2.90643997 5.04012427 3.21843767 6.00366358 4.31195651\n",
      "  4.55132835 1.57678137 5.12620195 0.         6.81994196 8.63669609]\n",
      " [5.36017682 2.09606084 0.92428586 3.383868   6.13404561 0.81684487\n",
      "  0.4585373  4.13502948 1.94321825 3.82303178 2.26087161 2.72951295\n",
      "  2.54657372 5.90649367 1.7360768  6.81994196 0.         2.48649457]\n",
      " [6.99961064 4.58255541 3.41078043 4.97909463 8.07999302 2.20726691\n",
      "  2.59315409 5.73025612 4.33989524 5.41825842 4.34577464 4.32473958\n",
      "  4.14180036 7.55357304 4.17297074 8.63669609 2.48649457 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "w_distances_b = compute_wasserstein_distances(persistence_diagrams_1b)\n",
    "print(w_distances_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -3.17018271 -3.73248722 -1.54940731 -0.89005418 -4.12714901\n",
      "  -4.60344261 -0.62295936 -3.32390343 -0.84109185 -4.06745053 -2.01163125\n",
      "  -2.25614815 -2.42701291 -3.33699094 -1.12257596 -4.73664314 -6.35394535]\n",
      " [-3.17018271  0.         -0.56502138 -1.53892349 -3.62075542 -1.9994606\n",
      "  -1.50241232 -1.84650052 -0.31756715 -1.46262804 -2.16144119 -0.43990377\n",
      "  -1.29299257 -3.72184153 -0.77787828 -4.11120679 -1.51248987 -3.93866629]\n",
      " [-3.73248722 -0.56502138  0.         -1.97743985 -4.34424579 -0.99123171\n",
      "  -1.02638957 -3.04756148 -0.58176929 -2.57693319 -2.16953961 -0.68052208\n",
      "  -1.72744371 -4.67379047 -0.22469497 -5.47158219 -0.76524405 -3.21612499]\n",
      " [-1.54940731 -1.53892349 -1.97743985  0.         -2.63524371 -2.51233456\n",
      "  -3.11795007 -0.69694902 -1.60525886 -0.21990203 -3.73764589 -0.67707716\n",
      "  -0.67319914 -4.08857587 -2.36054342 -3.34789887 -2.9091783  -4.33397915]\n",
      " [-0.89005418 -3.62075542 -4.34424579 -2.63524371  0.         -5.20820734\n",
      "  -5.31857446 -1.62682019 -3.66972052 -1.46976559 -4.83899817 -2.93149808\n",
      "  -2.98485896 -2.13684968 -4.17621981 -1.2239771  -5.2149847  -7.07680227]\n",
      " [-4.12714901 -1.9994606  -0.99123171 -2.51233456 -5.20820734  0.\n",
      "  -0.77233577 -3.21882857 -1.70947865 -2.52353583 -2.50863003 -1.05397852\n",
      "  -1.50511823 -5.08140668 -1.66171484 -6.19144486 -0.36301999 -1.72474879]\n",
      " [-4.60344261 -1.50241232 -1.02638957 -3.11795007 -5.31857446 -0.77233577\n",
      "   0.         -3.98154133 -1.10942425 -3.42691806 -1.93790726 -1.80790491\n",
      "  -2.44185284 -5.3215133  -1.55735099 -6.56596933 -0.31927506 -2.24573243]\n",
      " [-0.62295936 -1.84650052 -3.04756148 -0.69694902 -1.62682019 -3.21882857\n",
      "  -3.98154133  0.         -2.20184744 -0.11693188 -3.67065903 -0.39144236\n",
      "  -1.36439923 -3.06707937 -1.89580964 -2.65737852 -3.98536524 -5.46219643]\n",
      " [-3.32390343 -0.31756715 -0.58176929 -1.60525886 -3.66972052 -1.70947865\n",
      "  -1.10942425 -2.20184744  0.         -2.21608275 -2.62665524 -0.76624357\n",
      "  -1.695633   -4.17916113 -1.54732341 -4.37315722 -1.41851301 -3.88294904]\n",
      " [-0.84109185 -1.46262804 -2.57693319 -0.21990203 -1.46976559 -2.52353583\n",
      "  -3.42691806 -0.11693188 -2.21608275  0.         -3.5339484   0.11049542\n",
      "  -1.30872547 -3.26873363 -1.65329073 -2.60672145 -3.53672746 -5.21332087]\n",
      " [-4.06745053 -2.16144119 -2.16953961 -3.73764589 -4.83899817 -2.50863003\n",
      "  -1.93790726 -3.67065903 -2.62665524 -3.5339484   0.         -1.93118111\n",
      "  -3.17767556 -4.52595195 -1.26757531 -5.64253126 -2.16027204 -4.18256072]\n",
      " [-2.01163125 -0.43990377 -0.68052208 -0.67707716 -2.93149808 -1.05397852\n",
      "  -1.80790491 -0.39144236 -0.76624357  0.11049542 -1.93118111  0.\n",
      "  -0.33985907 -2.21270314 -0.67524413 -3.13595883 -1.52227182 -3.21248359]\n",
      " [-2.25614815 -1.29299257 -1.72744371 -0.67319914 -2.98485896 -1.50511823\n",
      "  -2.44185284 -1.36439923 -1.695633   -1.30872547 -3.17767556 -0.33985907\n",
      "   0.         -3.81926603 -1.19833932 -4.02177775 -2.34243502 -4.06597138]\n",
      " [-2.42701291 -3.72184153 -4.67379047 -4.08857587 -2.13684968 -5.08140668\n",
      "  -5.3215133  -3.06707937 -4.17916113 -3.26873363 -4.52595195 -2.21270314\n",
      "  -3.81926603  0.         -3.93596314 -1.11203364 -5.46134127 -7.15855504]\n",
      " [-3.33699094 -0.77787828 -0.22469497 -2.36054342 -4.17621981 -1.66171484\n",
      "  -1.55735099 -1.89580964 -1.54732341 -1.65329073 -1.26757531 -0.67524413\n",
      "  -1.19833932 -3.93596314  0.         -4.65444001 -1.25992442 -3.61621851]\n",
      " [-1.12257596 -4.11120679 -5.47158219 -3.34789887 -1.2239771  -6.19144486\n",
      "  -6.56596933 -2.65737852 -4.37315722 -2.60672145 -5.64253126 -3.13595883\n",
      "  -4.02177775 -1.11203364 -4.65444001  0.         -6.49453006 -8.11957494]\n",
      " [-4.73664314 -1.51248987 -0.76524405 -2.9091783  -5.2149847  -0.36301999\n",
      "  -0.31927506 -3.98536524 -1.41851301 -3.53672746 -2.16027204 -1.52227182\n",
      "  -2.34243502 -5.46134127 -1.25992442 -6.49453006  0.         -2.25168458]\n",
      " [-6.35394535 -3.93866629 -3.21612499 -4.33397915 -7.07680227 -1.72474879\n",
      "  -2.24573243 -5.46219643 -3.88294904 -5.21332087 -4.18256072 -3.21248359\n",
      "  -4.06597138 -7.15855504 -3.61621851 -8.11957494 -2.25168458  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(w_distances - w_distances_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of negative entries below the diagonal:  99.34640522875817\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def count_negative_entries_below_diagonal(matrix):\n",
    "    count = 0\n",
    "    total = 0\n",
    "    n = len(matrix)\n",
    "    for i in range(n):\n",
    "        for j in range(i):\n",
    "            if matrix[i][j] < 0:\n",
    "                count += 1\n",
    "            total += 1\n",
    "    return count, total\n",
    "\n",
    "# example matrix\n",
    "matrix = w_distances - w_distances_b\n",
    "\n",
    "negative_count, total_count = count_negative_entries_below_diagonal(matrix)\n",
    "percentage = (negative_count / total_count) * 100\n",
    "print(\"Percentage of negative entries below the diagonal: \", percentage)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the results\n",
    "\n",
    "So, as we can see, the percentage of negaive entries below the diagonal is approximately $99.35\\%$, meaning $99.35\\%$ of the time the Wasserstein distance between persistence diagrams for the phrase \"לצפות בזרחת השמש\" in various different contexts (given by the text corpera at the beginning of the notebook) is lower for `TurkuNLP/wikibert-base-he-cased` than for `xlm-roberta-large`. This means that the `TurkuNLP/wikibert-base-he-cased` model preserves the persistent homology for the context vectors associated to the phrase \"לצפות בזרחת השמש\" better in more contexts. Note, this is comparing `head 3` of `layer 7` in the models. When we compare different heads this number will change and some heads drastically outperform others at preserving persistent homology of keyphrases. \n",
    "\n",
    "It is interesting to note that we might linguistically analyze a model's individual attention heads using this method to determine what kinds of keyphrases it preserves. We might also use this to modify attention heads to better understand certain keyphrases. This is closely linked to topic modeling as is mentioned in [Topics in Contextualised Attention Embeddings](https://arxiv.org/pdf/2301.04339.pdf). This, along with the Fréchet mean of persistence diagrams could also be used for anomaly detection, where an anomaly is considered a \"significant\" deviation from the Fréchet mean persistence diagram. For character level transformers this might also be used as a topological prior to obtain something like hierarchical morphological segmentations of words, where the hierarchy is given by the simplex tree, and the simplex tree is encouraged to mimic heirarchical morphological segmentation of words similar to what is described in [Morphological Segmentation Inside-Out](https://arxiv.org/abs/1911.04916v2). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
